<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[logistic regression]]></title>
    <url>%2F2018%2F07%2F16%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[第三章 使用sklearn 实现机学习的分类算法分类算法 分类器的性能与计算能力和预测性能很大程度上取决于用于模型训练的数据 训练机器学习算法的五个步骤： 特征的选择 确定评价性能的标准 选择分类器及其优化算法 对模型性能的评估 算法的调优 sklearn初步使用 3.1 sklearn中包括的processing 模块中的标准化类，StandardScaler对特征进行标准化处理123456789from sklearn.processing import StandardSaclersc = StandardScaler() #实例化sc.fit(X_train)sc.transform(X_train)# - 以上两句可以并写成一句sc.fit_transform(X_trian)# - 我们使用相同的放缩参数分别对训练和测试数据集以保证他们的值是彼此相当的。**但是在使用fit_transform 只能对训练集使用，而测试机则只使用fit即可。**# - sklearn中的metrics类中包含了很多的评估参数，其中accuracy_score,# - 中accuracy_score(y_test,y_pred)，也就是那y_test与预测值相比较，得出正确率y_pred = model.predict(X_test-std) 过拟合现象过拟合现象出现有两个原因： 训练集与测试集特征分布不一致（黑天鹅和白天鹅） 模型训练的太过复杂，而样本量不足。同时针对两个原因而出现的解决方法: 收集多样化的样本 简化模型 交叉检验 逻辑斯谛回归感知机的一个最大缺点是：在样本不是完全线性可分的情况下，它永远不会收敛。分类算中的另一个简单高效的方法：logistics regression（分类模型） 很多情况下，我们会将逻辑回归的输出映射到二元分类问题的解决方案，需要确保逻辑回归的输出始终落在在0-1之间，此时S型函数的输出值正好满足了这个条件，其中： 几率比（odd ratio）特定的事件的发生的几率，用数学公式表示为：$\frac{p}{1-p} $，其中p为正事件的概率，不一定是有利的事件，而是我们将要预测的事件。以一个患者患有某种疾病的概率，我们可以将正事件的类标标记为y=1。 也就是样本特征与权重的线性组合，其计算公式： z = w·x + b 预测得到的概率可以通过一个量化器（单位阶跃函数）简单的转化为二元输出 如果y＞0.5 则判断该样本类别为1，如y＜0.5，则判定该样本是其他类别。 对应上面的展开式，如果z≥0，则判断类别是1，否则是其他。 阈值也就是0.5 通过逻辑斯谛回归模型的代价函数获得权重 判定某个样本属于类别1或者0 的条件概率如下： 逻辑回回归的代价函数是最小二乘损失函数 为了推导出逻辑斯蒂回归的代价函数，需要先定义一个极大似然函数L, 用极大似然估计来根据给定的训练集估计出参数w,对上式两边取对数，化简为求极大似然函数的最大值等价于求-l(w)的最小值，即： 利用梯度下降法求参数 在开始梯度下降之前，sigmoid function有一个很好的性质，梯度的负方向就是代价函数下降最快的方向，借助泰勒展开，可以得到（函数可微，可导）其中，f’(x) 和δ为向量，那么这两者的内积就等于当θ=π时，也就是在δ的f’(x)的负方向时，取得最小值， 也就是下降的最快的方向了也就是 其中，wj表示第j个特征的权重，η为学习率，用来控制步长。 对损失函数J(θ)中的θ的第j个权重求偏导，所以，在使用梯度下降法更新权重时，只要根据公式当样本量极大的时候，每次更新权重需要耗费大量的算力，这时可以采取随机梯度下降法，这时，每次迭代的时候需要将样本重新打乱，然后用下面的式子更新权重 参考文献: Raschka S. Python Machine Learning[M]. Packt Publishing, 2015 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>logistic regression</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[general regression models]]></title>
    <url>%2F2018%2F07%2F11%2Fgeneral-regression-models%2F</url>
    <content type="text"><![CDATA[1.1.1. Ordinary Least Squares123from sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit([[0,0],[1,1],[2,2]],[0,1,2]) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1reg.coef_ array([0.5, 0.5]) 最小二乘法的代价函数表述为： 1.1.2 Ridge Regression岭回归通过对最小二乘法的系数做出惩罚以解决部分的问题，最小化了惩罚的残差平方和 现行回归含有惩罚项的代价函数表述为： 正则化的背后的概念是引入额外的信息（偏差）来对极端参数的权重做出惩罚，此处的正则化则是引入的L2正则化。 代价函数的参数α的变化导致权重稀疏的变化，岭回归即L2正则化（L2收缩），也叫权重衰减： 123from sklearn import linear_modelreg= linear_model.Ridge(alpha=0.5)reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&apos;auto&apos;, tol=0.001) 1reg.intercept_ 0.1363636363636364 1reg.coef_ array([0.34545455, 0.34545455]) 1.1.2.1 Setting the regularization parameter: generalized Cross-Validation 通过交叉验证获得回归效果最恰当的惩罚项的参数123from sklearn import linear_modelreg=linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None, store_cv_values=False) 1reg.alpha_ 0.1 1.1.3 Lasso(权重稀疏） L1正则化可生成稀疏的特征向量，且大多数的权值为0，当高维的数据集中包含许多不想管的特征，尤其是在不相关的特征数量大于样本数量是，权重的稀疏化可以发挥特征选择的作用。 损失函数可以表示为： 123from sklearn import linear_modelreg= linear_model.Lasso(alpha=0.1)reg.fit([[0,0],[1,1]],[0,1]) Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 123import numpy as npreg.predict([[1,1]])# 如果是一维数组的话，需要在外面再加一层中括号，或者 array([0.8]) 1reg Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 1reg.coef_ array([0.6, 0. ]) 1reg.intercept_ 0.2 12]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging和Boosting的概念与区别]]></title>
    <url>%2F2018%2F06%2F11%2FHello-hexo%2F</url>
    <content type="text"><![CDATA[Bagging和Boosting的概念与区别随机森林属于集成学习(ensemble learning)中的bagging算法，在集成算法中主要分为bagging算法与boosting算法 Bagging算法(套袋法，bootstrap aggregating) bagging的算法过程如下： 从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。 对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是决策树，knn等） 对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。 Boosting（提升法） boosting的算法过程如下： 对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。 同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。) Bagging和Boosting 的主要区别 样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样的权重。 样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大 预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。 并行计算: Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成. 将决策树与以上框架组合成新的算法 Bagging + 决策树 = 随机森林 AdaBoost + 决策树 = 提升树 gradient + 决策树 = （梯度提升树）GDBT 决策树 常用的决策树有ID3， C4.5 ,CART三种. 三种算法模型构架相似，只是采用了不同的指标 首先看ID3算法 基于奥卡姆剃刀原理，即用尽量较少的东西做更多的事。ID3算法即iterative Dichotomiser3，迭代二叉树三代，越是小型的决策树优于较大的决策树。 核心思想是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分类。 信息增益是属性选择中一个重要指标，它定义为一个属性能够为分类系统带来的多少信息，带来的信息越多，该属性就越重要，而信息量，就是熵。 熵的定义是信息量的期望值，熵越大，一个变量的不确定性越大，它带来的信息量就越大，计算信息熵的公式为：，其中，p为出现c分类时的概率。 如何计算一个属性的信息增益？]]></content>
      <categories>
        <category>ensemble method</category>
      </categories>
      <tags>
        <tag>boosting</tag>
        <tag>集成算法</tag>
      </tags>
  </entry>
</search>
