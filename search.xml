<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[美团某商家的评论销售数据分析]]></title>
    <url>%2F2018%2F08%2F11%2F%E7%BE%8E%E5%9B%A2%E6%9F%90%E5%95%86%E5%AE%B6%E7%9A%84%E8%AF%84%E8%AE%BA%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[基于pandas python的美团某商家的评论销售数据分析 爬虫获得美团某家烧烤店的订单（代码省略） 数据清洗 数据初步的统计 可视化 数据中的评论数据用于自然语言处理（待续） 导入相关库12345from pyecharts import Bar,Pieimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport time 数据清洗与简单统计 评论数据，其中包括一下几个字段 是否匿名，均价，评价（以去掉，后续会做一些关于这些评论的更为深入的分析），评价时间，交易截止时间，订单号，套餐，上传的图片链接，质量好坏，阅读量，回复量，评分，点赞数等。 1df=pd.read_excel("all_data_meituan.xlsx") 1df.drop('comment',axis=1).head(2) 12df['avgPrice'].value_counts()# 同一家店的均价应该为同一个数值，所以这列数据没多大的意义 73 17400 Name: avgPrice, dtype: int64 12df['anonymous'].value_counts()# 匿名评价与实名评价的比例大致在5:1左右 False 14402 True 2998 Name: anonymous, dtype: int64 时间格式的转化123456def convertTime(x): y=time.localtime(x/1000) z=time.strftime("%Y-%m-%d %H:%M:%S",y) return zdf["commentTime"]=df["commentTime"].apply(convertTime)df["commentTime"].head() 0 2018-05-09 22:21:48 1 2018-06-01 19:41:31 2 2018-04-04 11:52:23 3 2018-05-01 17:12:22 4 2018-05-17 16:48:04 Name: commentTime, dtype: object 123# 在excel可以用筛选器直接看到这列中的数据含有缺失值，或者在拿到数据的时候，使用df.info() 查看每列的数据信息情况df['dealEndtime'].isna().value_counts()# 这列数据中含有177个缺失值，其余完整 False 17223 True 177 Name: dealEndtime, dtype: int64 月统计 12345678910df['commentTime']=pd.to_datetime(df['commentTime'])df1 = df.set_index('commentTime')df1.resample('D').size().sort_values(ascending=False).head(100)df2=df1.resample('M').size().to_period()df2=df2.reset_index()# df2.columns# from pyecharts import Barbar =Bar("按月统计",width=1000,height=800)bar.add("月统计表",df2['commentTime'],df2[0],is_label_show=True, is_datazoom_show=True,is_toolbox_show=True,is_more_utils=True)bar 天统计 12345678df['commentTime']=pd.to_datetime(df['commentTime'])df['hour'] = df['commentTime'].dt.hourdf2= df.groupby(['hour']).size()df2from pyecharts import Barbar =Bar("分时统计",width=1000,height=600)bar.add("分时计表",['&#123;&#125; h'.format(i) for i in df2.index],df2.values,is_label_show=True, is_datazoom_show=True,is_toolbox_show=True,is_more_utils=True,is_random=True)bar 周统计123456789df['commentTime']=pd.to_datetime(df['commentTime'])df['weekday'] = df['commentTime'].dt.weekdaydf2= df.groupby(['weekday']).size()# 周末吃外卖的还是教平时多了一些from pyecharts import Barbar =Bar("周总计",width=750,height=400)weekday=["一","二","三","四","五","六","日"]bar.add("周总计",['周&#123;&#125;'.format(i) for i in weekday],df2.values,is_label_show=True, is_datazoom_show=False,is_toolbox_show=True,is_more_utils=True,is_random=True)bar 123# 处理数据前需要先处理缺失值# 订单结束时间清洗df['dealEndtime'].fillna(method='ffill').apply(lambda x:time.strftime("%Y-%m-%d %H:%M:%S",time.localtime(x))).head() 0 2018-06-30 14:00:00 1 2018-06-30 14:00:00 2 2018-06-30 14:00:00 3 2018-06-30 14:00:00 4 2018-06-30 14:00:00 Name: dealEndtime, dtype: object 套餐的统计1df['menu'].dropna().astype('category').value_counts() 2人午晚餐 7640 单人午晚餐 3920 学生专享午晚自助 2638 4人午/晚自助 1581 单人下午自助烤肉 639 6人午/晚自助 507 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 209 单人午/晚自助 67 周一至周五自助烤肉，免费WiFi 22 Name: menu, dtype: int64 阅读数与评分的协方差（相关性） 12df['readCnt'].corr(df['star'])# 评论阅读书与客户评价分数高低的相关性 0.05909293203205019 最受欢迎的套餐(2人午晚餐评价分布)，基本上几种在30,40,50,评价都还好，怪不得卖得好 1df_most=df[(df["menu"]=="2人午晚餐")]['star'].value_counts().reindex(range(10,60,10)) 10 329 20 533 30 2002 40 2704 50 2072 Name: star, dtype: int64 1df[(df["menu"]=="单人午晚餐")]['star'].value_counts() 30 1215 40 1208 50 1093 20 298 10 106 Name: star, dtype: int64 12# 学生专享午晚自助 df[(df["menu"]=="学生专享午晚自助")]['star'].value_counts() 40 954 50 863 30 529 20 191 10 101 Name: star, dtype: int64 1df[(df["menu"]=="4人午/晚自助")]['star'].value_counts() 50 536 30 432 40 414 10 131 20 68 Name: star, dtype: int64 1df[(df["menu"]=="单人下午自助烤肉")]['star'].value_counts() 30 208 50 169 40 144 10 98 20 20 Name: star, dtype: int64 1df[(df["menu"]=="6人午/晚自助")]['star'].value_counts() 50 245 40 142 30 112 10 8 Name: star, dtype: int64 12#周一至周五自助烤肉/周六日及节假日自助烤肉2选1df[(df["menu"]=="周一至周五自助烤肉/周六日及节假日自助烤肉2选1")]['star'].value_counts() 50 87 40 66 30 46 20 10 Name: star, dtype: int64 1df[(df["menu"]=="单人午/晚自助")]['star'].value_counts() 50 30 40 27 30 10 Name: star, dtype: int64 1df[(df["menu"]=="周一至周五自助烤肉，免费WiFi")]['star'].value_counts().reindex(range(10,51,10)).fillna(0) 10 0.0 20 0.0 30 0.0 40 0.0 50 22.0 Name: star, dtype: float64 套餐与评价汇总12# df.groupby(['menu','star']).size().to_excel("all_menu_star.xls") 可以直接导出到exceldf.groupby(['menu','star']).size() menu star 2人午晚餐 10 329 20 533 30 2002 40 2704 50 2072 4人午/晚自助 10 131 20 68 30 432 40 414 50 536 6人午/晚自助 10 8 30 112 40 142 50 245 单人下午自助烤肉 10 98 20 20 30 208 40 144 50 169 单人午/晚自助 30 10 40 27 50 30 单人午晚餐 10 106 20 298 30 1215 40 1208 50 1093 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 20 10 30 46 40 66 50 87 周一至周五自助烤肉，免费WiFi 50 22 学生专享午晚自助 10 101 20 191 30 529 40 954 50 863 dtype: int64 1df.groupby(['star','menu',]).size() star menu 10 2人午晚餐 329 4人午/晚自助 131 6人午/晚自助 8 单人下午自助烤肉 98 单人午晚餐 106 学生专享午晚自助 101 20 2人午晚餐 533 4人午/晚自助 68 单人下午自助烤肉 20 单人午晚餐 298 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 10 学生专享午晚自助 191 30 2人午晚餐 2002 4人午/晚自助 432 6人午/晚自助 112 单人下午自助烤肉 208 单人午/晚自助 10 单人午晚餐 1215 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 46 学生专享午晚自助 529 40 2人午晚餐 2704 4人午/晚自助 414 6人午/晚自助 142 单人下午自助烤肉 144 单人午/晚自助 27 单人午晚餐 1208 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 66 学生专享午晚自助 954 50 2人午晚餐 2072 4人午/晚自助 536 6人午/晚自助 245 单人下午自助烤肉 169 单人午/晚自助 30 单人午晚餐 1093 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 87 周一至周五自助烤肉，免费WiFi 22 学生专享午晚自助 863 dtype: int64 评分最高的套餐分布 1df.groupby(['star','menu',]).size()[50] menu 2人午晚餐 2072 4人午/晚自助 536 6人午/晚自助 245 单人下午自助烤肉 169 单人午/晚自助 30 单人午晚餐 1093 周一至周五自助烤肉/周六日及节假日自助烤肉2选1 87 周一至周五自助烤肉，免费WiFi 22 学生专享午晚自助 863 dtype: int64 用户id统计 123# userId# 这家店铺有好多回头客，万万没想到df[df['userId']!=0]['userId'].value_counts().head(40) 266045270 64 152775497 60 80372612 60 129840082 60 336387962 60 34216474 60 617772217 60 82682689 54 287219504 49 884729389 45 ... 232697160 40 141718492 40 879430090 40 696143486 40 13257519 40 983797146 40 911947863 40 993057629 40 494215297 40 Name: userId, dtype: int64 用户名统计,应该与用户id对应 1df[df['userName']!="匿名用户"]['userName'].value_counts().head(40) xuruiss1026 64 黑发飘呀飘 60 么么哒我是你聪叔 60 jIx325233926 60 siisgood 60 vTF610712604 60 始于初见的你 60 yumengkou 54 Daaaav 49 梁子7543 45 oev575457132 40 oUI806055883 40 joF498901567 40 liE32679330 40 ... 清晨cxh98 40 cBj31240225 40 天蛟Wing 40 榴莲馅月饼 40 leeman666888 40 迅行天下 40 滨海之恋33 40 pHO437742850 40 SzX539077433 40 Name: userName, dtype: int64 评分与用户等级汇总1df.groupby(['star','userLevel',]).size() star userLevel 10 0 187 1 139 2 164 3 193 4 80 5 10 20 0 223 1 88 2 304 3 294 4 207 5 21 30 0 1147 1 405 2 1057 3 1230 4 570 5 165 6 20 40 0 870 1 432 2 1360 3 1751 4 1026 5 261 6 25 50 0 698 1 386 2 1167 3 1670 4 802 5 318 6 130 dtype: int64 12345678910111213141516171819202122df_level_star = df.groupby(['userLevel','star']).size()attr = np.arange(10,60,10)from pyecharts import Barbar = Bar("用户等级与评分",title_pos="center")df_0 = df_level_star[0].valuesdf_1 = df_level_star[1].valuesdf_2 = df_level_star[2].valuesdf_3 = df_level_star[3].valuesdf_4 = df_level_star[4].valuesdf_5 = df_level_star[5].values# df_6 = df_level_star[6].valuesdf_6 = df_level_star[6].reindex(attr).fillna(0).valuesbar.add("level 0",attr,df_0,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 1",attr,df_1,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 2",attr,df_2,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 3",attr,df_3,mark_line=["average"],mark_point=['max','min'],is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 4",attr,df_4,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 5",attr,df_5,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 6",attr,df_6,is_label_show=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar 123456789101112131415161718bar = Bar("用户等级与评分",title_pos="center",title_color="red")attr = np.arange(10,60,10)df_0 = df_level_star[0].valuesdf_1 = df_level_star[1].valuesdf_2 = df_level_star[2].valuesdf_3 = df_level_star[3].valuesdf_4 = df_level_star[4].valuesdf_5 = df_level_star[5].values# df_6 = df_level_star[6].valuesdf_6 = df_level_star[6].reindex(attr).fillna(0).valuesbar.add("level 0",attr,df_0,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 1",attr,df_1,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 2",attr,df_2,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 3",attr,df_3,is_stack=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 4",attr,df_4,is_stack=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 5",attr,df_5,is_stack=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar.add("level 6",attr,df_6,is_stack=True,legend_pos='right',legend_orient='vertical',label_text_size=12)bar 用户等级与评价的相关性 1df['star'].corr(df['userLevel']) 0.14389808871897794 点赞分布 12345df_zan=df['zanCnt'].value_counts()from pyecharts import Barbar=Bar("点赞统计")bar.add("点赞分布",df_zan.index[1:],df_zan.values[1:],is_label_show=True)bar 数值型数据的统计 1df.describe() 1df['userLevel'].value_counts().reindex(range(7)) 0 3125 1 1450 2 4052 3 5138 4 2685 5 775 6 175 Name: userLevel, dtype: int64 用户等级分布 12345df_level=df['userLevel'].value_counts().reindex(range(7))from pyecharts import Piepie=Pie("用户等级分布",title_pos="center",width=900)pie.add("levels distribution",["level "+str(i) for i in range(7)],df_level.values,is_random=True,radidus=[30,45],legend_pos='left',rosetype='area',legend_orient='vertical',is_label_show=True,label_text_size=20)pie]]></content>
      <categories>
        <category>pandas</category>
        <category>visualization</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>data visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[douban_top250_analysis]]></title>
    <url>%2F2018%2F08%2F08%2Fdouban-top250-analysis%2F</url>
    <content type="text"><![CDATA[豆瓣电影top250数据分析 数据来源（豆瓣电影top250） 爬虫代码比较简单 数据较为真实，可以进行初步的数据分析 可以将前面的几篇文章中的介绍的数据预处理的方法进行实践 最后用matplotlib与pyecharts两种可视化包进行部分数据展示 数据仍需深挖，有待加强 12345#首先按照惯例导入python 数据分析的两个包import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom pyecharts import Bar 数据预处理12345names=['num','title',"director","role","init_year","area","genre","rating_num","comment_num","comment","url"]#"num#title#director#role#init_year#area#genre#rating_num#comment_num#comment#url"df_1 = pd.read_excel("top250_f1.xls",index=None,header=None)df_1.columns=namesdf_1.head() 查看数据类型 1df_1.dtypes num int64 title object director object role object init_year object area object genre object rating_num float64 comment_num int64 comment object url object dtype: object 1234names1=["num","rank","alt_title","title","pubdate","language","writer","director","cast","movie_duration","year","movie_type","tags","image"]df_2 = pd.read_excel("top250_f2.xlsx",index=None,header=None)df_2.columns=names1df_2.info() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 250 entries, 0 to 249 Data columns (total 14 columns): num 250 non-null int64 rank 250 non-null float64 alt_title 250 non-null object title 250 non-null object pubdate 250 non-null object language 250 non-null object writer 250 non-null object director 250 non-null object cast 250 non-null object movie_duration 250 non-null object year 250 non-null object movie_type 250 non-null object tags 250 non-null object image 250 non-null object dtypes: float64(1), int64(1), object(12) memory usage: 27.4+ KB 1234df_1_cut = df_1[['num','title','init_year','area','genre','rating_num','comment_num']]df_2_cut = df_2[['num','language','director','cast','movie_duration','tags']]df = pd.merge(df_1_cut,df_2_cut,how = 'outer',on = 'num') #外连接，合并标准on = 'num'# df.to_excel("all_data_movie.xls",index=False) #查看前五条信息 查看重复数据 123# 查看重复数据df.duplicated()df.duplicated().value_counts() False 250 dtype: int64 1df.title.unique() array([&apos;肖申克的救赎&apos;, &apos;霸王别姬&apos;, &apos;这个杀手不太冷&apos;, &apos;阿甘正传&apos;, &apos;美丽人生&apos;, &apos;泰坦尼克号&apos;, &apos;千与千寻&apos;, &apos;辛德勒的名单&apos;, &apos;盗梦空间&apos;, &apos;机器人总动员&apos;, &apos;三傻大闹宝莱坞&apos;, &apos;忠犬八公的故事&apos;, &apos;海上钢琴师&apos;, &apos;放牛班的春天&apos;, &apos;大话西游之大圣娶亲&apos;, &apos;楚门的世界&apos;, &apos;教父&apos;, &apos;龙猫&apos;, &apos;星际穿越&apos;, &apos;熔炉&apos;, &apos;触不可及&apos;, &apos;无间道&apos;, &apos;乱世佳人&apos;, &apos;当幸福来敲门&apos;, &apos;怦然心动&apos;, &apos;天堂电影院&apos;, &apos;十二怒汉&apos;, &apos;鬼子来了&apos;, &apos;蝙蝠侠：黑暗骑士&apos;, &apos;疯狂动物城&apos;, &apos;少年派的奇幻漂流&apos;, &apos;活着&apos;, &apos;搏击俱乐部&apos;, &apos;指环王3：王者无敌&apos;, &apos;天空之城&apos;, &apos;大话西游之月光宝盒&apos;, &apos;飞屋环游记&apos;, &apos;罗马假日&apos;, &apos;控方证人&apos;, &apos;窃听风暴&apos;, &apos;两杆大烟枪&apos;, &apos;飞越疯人院&apos;, &apos;闻香识女人&apos;, &apos;哈尔的移动城堡&apos;, &apos;辩护人&apos;, &apos;海豚湾&apos;, &apos;V字仇杀队&apos;, &apos;死亡诗社&apos;, &apos;摔跤吧！爸爸&apos;, &apos;教父2&apos;, &apos;指环王2：双塔奇兵&apos;, &apos;美丽心灵&apos;, &apos;指环王1：魔戒再现&apos;, &apos;饮食男女&apos;, &apos;情书&apos;, &apos;美国往事&apos;, &apos;狮子王&apos;, &apos;素媛&apos;, &apos;钢琴家&apos;, &apos;小鞋子&apos;, &apos;七宗罪&apos;, &apos;天使爱美丽&apos;, &apos;被嫌弃的松子的一生&apos;, &apos;致命魔术&apos;, &apos;本杰明·巴顿奇事&apos;, &apos;音乐之声&apos;, &apos;西西里的美丽传说&apos;, &apos;勇敢的心&apos;, &apos;拯救大兵瑞恩&apos;, &apos;黑客帝国&apos;, &apos;低俗小说&apos;, &apos;剪刀手爱德华&apos;, &apos;让子弹飞&apos;, &apos;看不见的客人&apos;, &apos;沉默的羔羊&apos;, &apos;蝴蝶效应&apos;, &apos;入殓师&apos;, &apos;大闹天宫&apos;, &apos;春光乍泄&apos;, &apos;末代皇帝&apos;, &apos;心灵捕手&apos;, &apos;玛丽和马克思&apos;, &apos;阳光灿烂的日子&apos;, &apos;哈利·波特与魔法石&apos;, &apos;布达佩斯大饭店&apos;, &apos;幽灵公主&apos;, &apos;第六感&apos;, &apos;禁闭岛&apos;, &apos;重庆森林&apos;, &apos;猫鼠游戏&apos;, &apos;狩猎&apos;, &apos;致命ID&apos;, &apos;大鱼&apos;, &apos;断背山&apos;, &apos;甜蜜蜜&apos;, &apos;射雕英雄传之东成西就&apos;, &apos;告白&apos;, &apos;一一&apos;, &apos;加勒比海盗&apos;, &apos;穿条纹睡衣的男孩&apos;, &apos;阳光姐妹淘&apos;, &apos;摩登时代&apos;, &apos;阿凡达&apos;, &apos;上帝之城&apos;, &apos;爱在黎明破晓前&apos;, &apos;消失的爱人&apos;, &apos;风之谷&apos;, &apos;爱在日落黄昏时&apos;, &apos;侧耳倾听&apos;, &apos;超脱&apos;, &apos;倩女幽魂&apos;, &apos;恐怖直播&apos;, &apos;红辣椒&apos;, &apos;小森林 夏秋篇&apos;, &apos;喜剧之王&apos;, &apos;菊次郎的夏天&apos;, &apos;驯龙高手&apos;, &apos;幸福终点站&apos;, &apos;萤火虫之墓&apos;, &apos;借东西的小人阿莉埃蒂&apos;, &apos;岁月神偷&apos;, &apos;神偷奶爸&apos;, &apos;七武士&apos;, &apos;杀人回忆&apos;, &apos;贫民窟的百万富翁&apos;, &apos;电锯惊魂&apos;, &apos;喜宴&apos;, &apos;谍影重重3&apos;, &apos;真爱至上&apos;, &apos;怪兽电力公司&apos;, &apos;东邪西毒&apos;, &apos;记忆碎片&apos;, &apos;海洋&apos;, &apos;黑天鹅&apos;, &apos;雨人&apos;, &apos;疯狂原始人&apos;, &apos;卢旺达饭店&apos;, &apos;小森林 冬春篇&apos;, &apos;英雄本色&apos;, &apos;哈利·波特与死亡圣器(下)&apos;, &apos;燃情岁月&apos;, &apos;7号房的礼物&apos;, &apos;虎口脱险&apos;, &apos;心迷宫&apos;, &apos;萤火之森&apos;, &apos;傲慢与偏见&apos;, &apos;荒蛮故事&apos;, &apos;海边的曼彻斯特&apos;, &apos;请以你的名字呼唤我&apos;, &apos;教父3&apos;, &apos;恋恋笔记本&apos;, &apos;完美的世界&apos;, &apos;纵横四海&apos;, &apos;花样年华&apos;, &apos;唐伯虎点秋香&apos;, &apos;超能陆战队&apos;, &apos;玩具总动员3&apos;, &apos;蝙蝠侠：黑暗骑士崛起&apos;, &apos;时空恋旅人&apos;, &apos;魂断蓝桥&apos;, &apos;猜火车&apos;, &apos;穿越时空的少女&apos;, &apos;雨中曲&apos;, &apos;二十二&apos;, &apos;达拉斯买家俱乐部&apos;, &apos;我是山姆&apos;, &apos;人工智能&apos;, &apos;冰川时代&apos;, &apos;浪潮&apos;, &apos;朗读者&apos;, &apos;爆裂鼓手&apos;, &apos;香水&apos;, &apos;罗生门&apos;, &apos;未麻的部屋&apos;, &apos;阿飞正传&apos;, &apos;血战钢锯岭&apos;, &apos;一次别离&apos;, &apos;被解救的姜戈&apos;, &apos;可可西里&apos;, &apos;追随&apos;, &apos;恐怖游轮&apos;, &apos;撞车&apos;, &apos;战争之王&apos;, &apos;头脑特工队&apos;, &apos;地球上的星星&apos;, &apos;房间&apos;, &apos;无人知晓&apos;, &apos;梦之安魂曲&apos;, &apos;牯岭街少年杀人事件&apos;, &apos;魔女宅急便&apos;, &apos;谍影重重&apos;, &apos;谍影重重2&apos;, &apos;忠犬八公物语&apos;, &apos;模仿游戏&apos;, &apos;你的名字。&apos;, &apos;惊魂记&apos;, &apos;青蛇&apos;, &apos;一个叫欧维的男人决定去死&apos;, &apos;再次出发之纽约遇见你&apos;, &apos;哪吒闹海&apos;, &apos;完美陌生人&apos;, &apos;东京物语&apos;, &apos;小萝莉的猴神大叔&apos;, &apos;黑客帝国3：矩阵革命&apos;, &apos;源代码&apos;, &apos;新龙门客栈&apos;, &apos;终结者2：审判日&apos;, &apos;末路狂花&apos;, &apos;碧海蓝天&apos;, &apos;秒速5厘米&apos;, &apos;绿里奇迹&apos;, &apos;这个男人来自地球&apos;, &apos;海盗电台&apos;, &apos;勇闯夺命岛&apos;, &apos;城市之光&apos;, &apos;初恋这件小事&apos;, &apos;无耻混蛋&apos;, &apos;卡萨布兰卡&apos;, &apos;变脸&apos;, &apos;E.T. 外星人&apos;, &apos;爱在午夜降临前&apos;, &apos;发条橙&apos;, &apos;步履不停&apos;, &apos;黄金三镖客&apos;, &apos;无敌破坏王&apos;, &apos;疯狂的石头&apos;, &apos;美国丽人&apos;, &apos;荒野生存&apos;, &apos;迁徙的鸟&apos;, &apos;英国病人&apos;, &apos;海街日记&apos;, &apos;彗星来的那一夜&apos;, &apos;国王的演讲&apos;, &apos;非常嫌疑犯&apos;, &apos;血钻&apos;, &apos;燕尾蝶&apos;, &apos;聚焦&apos;, &apos;勇士&apos;, &apos;叫我第一名&apos;, &apos;穆赫兰道&apos;, &apos;遗愿清单&apos;, &apos;枪火&apos;, &apos;上帝也疯狂&apos;, &apos;我爱你&apos;, &apos;黑鹰坠落&apos;, &apos;荒岛余生&apos;, &apos;大卫·戈尔的一生&apos;, &apos;千钧一发&apos;, &apos;蓝色大门&apos;, &apos;2001太空漫游&apos;], dtype=object) 1234567# 数据格式的初步清洗df['genre']=df['genre'].str[2:-2]df["language"]= df['language'].str[2:-2]df["director"]= df['director'].str[2:-2]df["cast"]= df['cast'].str[2:-2]df["movie_duration"]= df['movie_duration'].str[2:-2]# df[["genre","language","director","cast","movie_duration"]]=df[["genre","language","director","cast","movie_duration"]].apply(lambda x: x.replace("['","").replace("']","")) 上映地区数据清理 1234567# 地区的数据清理area_split = df['area'].str.split(expand=True)area_split.head()all_area = area_split.apply(pd.value_counts).fillna(0)all_area.columns = ['area_1','area_2','area_3','area_4','area_5']all_area = all_area.astype("int")all_area.dtypes area_1 int32 area_2 int32 area_3 int32 area_4 int32 area_5 int32 dtype: object 1all_area.head() 上映地区数据整理 1all_area['Col_sum'] = all_area.apply(lambda x: x.sum(), axis=1) 1all_area.head() 电影类型数据清理 123categories = df['genre'].str.split(" ",expand=True)categories = categories.apply(pd.value_counts).fillna(0).astype("int")categories.head() 电影类型数据整理 123categories['count']= categories.apply(lambda x:x.sum(),axis=1)categories.sort_values('count',ascending=False)categories.head() 12# 对于language处理df['language'].head(10) 0 英语 1 汉语普通话 2 英语&apos;, &apos;意大利语&apos;, &apos;法语 3 英语 4 意大利语&apos;, &apos;德语&apos;, &apos;英语 5 英语&apos;, &apos;意大利语&apos;, &apos;德语&apos;, &apos;俄语 6 日语 7 英语&apos;, &apos;希伯来语&apos;, &apos;德语&apos;, &apos;波兰语 8 英语&apos;, &apos;日语&apos;, &apos;法语 9 英语 Name: language, dtype: object 电影语言的清洗 12language_all = df['language'].str.replace("\', \'"," ").str.split(" ",expand=True)language_all.head() 电影语言的数据整理 12language_all = language_all.apply(pd.value_counts).fillna(0).astype("int")language_all.head() 123language_all['count']= language_all.apply(lambda x:x.sum(),axis=1)language_all.sort_values('count',ascending=False)language_all.head() 1df.director.head() 0 弗兰克·德拉邦特 Frank Darabont 1 陈凯歌 Kaige Chen 2 吕克·贝松 Luc Besson 3 Robert Zemeckis 4 罗伯托·贝尼尼 Roberto Benigni Name: director, dtype: object 导演的数据整理 12director_all = df['director'].str.replace("\', \'","~").str.split("~",expand=True)director_all.head() 电影演员的数据清洗 12# 演员df['cast'].head() 0 蒂姆·罗宾斯 Tim Robbins&apos;, &apos;摩根·弗里曼 Morgan Freeman&apos;, ... 1 张国荣 Leslie Cheung&apos;, &apos;张丰毅 Fengyi Zhang&apos;, &apos;巩俐 Li... 2 让·雷诺 Jean Reno&apos;, &apos;娜塔莉·波特曼 Natalie Portman&apos;, &apos;加... 3 Tom Hanks&apos;, &apos;Robin Wright Penn&apos;, &apos;Gary Sinise&apos;... 4 罗伯托·贝尼尼 Roberto Benigni&apos;, &apos;尼可莱塔·布拉斯基 Nicoletta... Name: cast, dtype: object 12cast_all = df['cast'].str.replace("\', \'","~").str.split("~",expand=True)cast_all.head(2) 电影演员的数据整理 12345main_dr= list(director_all[0])second_dr= list(director_all[1])thrid_dr= list(director_all[2])directors=pd.Series(main_dr+second_dr+thrid_dr)directors.value_counts().head() 宫崎骏 Hayao Miyazaki 7 克里斯托弗·诺兰 Christopher Nolan 7 王家卫 Kar Wai Wong 5 史蒂文·斯皮尔伯格 Steven Spielberg 5 大卫·芬奇 David Fincher 4 dtype: int64 电影发行年份清洗 1df['init_year'].head() 0 1994 1 1993 2 1994 3 1994 4 1997 Name: init_year, dtype: object 123year_= df['init_year'].str.split('/').apply(lambda x:x[0].strip()).replace(regex=&#123;'\(中国大陆\)':''&#125;)year_split = pd.to_datetime(year_).dt.yearyear_split.head() 0 1994 1 1993 2 1994 3 1994 4 1997 Name: init_year, dtype: int64 电影观影时长的清洗 1df['movie_duration'].head() 0 142分钟 1 171 分钟 2 110分钟(剧场版)&apos;, &apos;133分钟(国际版) 3 142 分钟 4 116分钟&apos;, &apos;125分钟(加长版) Name: movie_duration, dtype: object 电影观影时长的整理 12345# 观影时长，多次上映取的第一个时长movie_duration_split = df['movie_duration'].str.replace("\', \'","~").str.split("~",expand=True).fillna(0)movie_duration_split =movie_duration_split.replace(regex=&#123;'分钟.*': ''&#125;)df['movie_duration']=movie_duration_split[0].astype("int")df['movie_duration'].head() 0 142 1 171 2 110 3 142 4 116 Name: movie_duration, dtype: int32 1234# 标签 tags# 查看第一部电影的的tag# pd.DataFrame(eval(df['tags'][0]))df['tags'][0] &quot;[{&apos;count&apos;: 220591, &apos;name&apos;: &apos;经典&apos;}, {&apos;count&apos;: 191014, &apos;name&apos;: &apos;励志&apos;}, {&apos;count&apos;: 173587, &apos;name&apos;: &apos;信念&apos;}, {&apos;count&apos;: 159939, &apos;name&apos;: &apos;自由&apos;}, {&apos;count&apos;: 115024, &apos;name&apos;: &apos;人性&apos;}, {&apos;count&apos;: 111430, &apos;name&apos;: &apos;美国&apos;}, {&apos;count&apos;: 93721, &apos;name&apos;: &apos;人生&apos;}, {&apos;count&apos;: 72602, &apos;name&apos;: &apos;剧情&apos;}]&quot; 电影标签的清洗 12345all_tags = [ pd.DataFrame(eval(i)) for i in df["tags"]]import itertoolsall_tags=[list(itertools.chain.from_iterable(zip(df_['name'],df_['count']))) for df_ in all_tags]all_tags_df=pd.DataFrame(all_tags)all_tags_df.head() 数据可视化部分12345678# 数据分析与可视化部分 matplotlib 与 pyechartsimport matplotlib.pyplot as pltimport matplotlibfrom pyecharts import Barimport seaborn as snsmatplotlib.rcParams["font.family"]=["simsunb"]matplotlib.rcParams['font.size'] =15 1234567891011plt.figure(figsize=(15,6))plt.style.use('seaborn-whitegrid')plt.subplot(1,2,1)plt.scatter(df['rating_num'],df['num'])plt.xlabel("reating_num")plt.ylabel("ranking")plt.gca().invert_yaxis()plt.subplot(1,2,2)plt.hist(df['rating_num'],bins=12)plt.xlabel("rating_num")plt.show() 123456789101112plt.figure(1)plt.figure(figsize=(15,6))plt.style.use('seaborn-whitegrid')plt.subplot(1,2,1)plt.scatter(df['movie_duration'],df['num'])plt.xlabel("movie_duration")plt.ylabel("ranking")plt.gca().invert_yaxis()plt.subplot(1,2,2)plt.hist(df['movie_duration'],bins=15)plt.xlabel("movie_duration")plt.show() 12# 观影时长与 电影排名之间的相关性，从常识来判断，基本没有啥关系，因为好的电影不一定时间长，时间长的不一定是好电影df['num'].corr(df['movie_duration']) -0.19979596696001942 12345678910111213df['init_year']=year_splitplt.figure(1)plt.figure(figsize=(15,6))plt.style.use('seaborn-whitegrid')plt.subplot(1,2,1)plt.scatter(df['init_year'],df['num'])plt.xlabel("init_year")plt.ylabel("ranking")plt.gca().invert_yaxis()plt.subplot(1,2,2)plt.hist(df['init_year'],bins=30)plt.xlabel("init_year")plt.show() 12df['num'].corr(df['init_year']) # 从结果来看，更没有什么相关性 0.041157240822869007 1234567891011# import matplotlib.font_manager as fm# fpath = 'C:\\Windows\\Fonts\\simsunb.ttf'# prop=fm.FontProperties(fname=fpath)# print(prop)matplotlib.rcParams["font.family"]=["SimHei"]plt.figure(figsize=(24,6))all_area_new = all_area['Col_sum'].sort_values(ascending=False)plt.bar(list(all_area_new.index),list(all_area_new))plt.xticks(rotation=45) #坐标轴刻度倾斜45°plt.legend(labels=["count"],loc='upper center')plt.show() 1language_all['count'].sort_values(ascending=False).head() 英语 170 法语 40 日语 40 汉语普通话 34 德语 24 Name: count, dtype: int64 123language_all['count'].sort_values(ascending=False).plot(kind='bar',figsize=(22,6))plt.legend(labels=["language_count"],loc='upper center')plt.show() 123categories["count"].sort_values(ascending=False).plot(kind='bar',figsize=(22,6))plt.legend(labels=["category_count"],loc='upper center')plt.show() 12all_tag_name = all_tags_df.loc[:,[0,2,4,6,8,10,12,14]].values.flatten()all_tag_name = pd.Series(all_tag_name).value_counts() 12345from pyecharts import WordCloudwordcloud = WordCloud(width=1000,height=600)wordcloud.add("",list(all_tag_name.index),list(all_tag_name.values),word_size_range=[20,100])wordcloud 1234567from wordcloud import WordCloudfont = r'C:\Windows\Fonts\simfang.ttf'wordcloud = WordCloud(font_path=font,max_font_size = 35).generate(str(list(all_tag_name.index)))plt.figure(figsize=(9,6))plt.imshow(wordcloud)plt.axis('off')plt.show() 12345from pyecharts import Barmybar= Bar("电影类型分析")cate=categories['count'].sort_values(ascending=False)mybar.add("电影类型",cate.index,cate.values,mark_line=['max'],mark_point=['average'])mybar 123456from pyecharts import PieTop30_rating_num=df[['rating_num','title']].sort_values(['rating_num'],ascending=False).head(30)['rating_num'].value_counts()Top30_rating_numpie = Pie('排名前30电影评分占比',title_pos = 'center')pie.add('',list(Top30_rating_num.index),Top30_rating_num.values,is_label_show = True,legend_orient = 'vertical',legend_pos = 'right')pie]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>data processing</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas data processing]]></title>
    <url>%2F2018%2F08%2F02%2Fdata-processing-pandas%2F</url>
    <content type="text"><![CDATA[category type data in pandas 实际应用pandas过程中，经常会用到category数据类型，通常以text的形式显示，包括颜色（红，绿，蓝），尺寸的大小（大，中，小），还有地理信息等（国家，省份），这些数据的处理经常会有各种各样的问题，pandas以及scikit-learn两个包可以将category数据转化为合适的数值型格式，这篇主要介绍通过这两个包处理category类型的数据转化为数值类型，也就是encoding的过程。 数据来源UCI Machine Learning Repository，这个数据集中包含了很多的category类型的数据，可以从链接汇总查看数据的代表的含义。 下面开始导入需要用到的包 12import numpy as npimport pandas as pd 12345678910# 规定一下数据列的各个名称，headers = ["symboling", "normalized_losses", "make", "fuel_type", "aspiration", "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base", "length", "width", "height", "curb_weight", "engine_type", "num_cylinders", "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price"]# 从pandas导入csv文件，将?标记为NaN缺失值df=pd.read_csv("http://mlr.cs.umass.edu/ml/machine-learning-databases/autos/imports-85.data",header=None,names=headers,na_values="?")df.head() 1df.dtypes symboling int64 normalized_losses float64 make object fuel_type object aspiration object num_doors object body_style object drive_wheels object engine_location object wheel_base float64 length float64 width float64 height float64 curb_weight int64 engine_type object num_cylinders object engine_size int64 fuel_system object bore float64 stroke float64 compression_ratio float64 horsepower float64 peak_rpm float64 city_mpg int64 highway_mpg int64 price float64 dtype: object 123# 如果只关注category 类型的数据，其实根本没有必要拿到这些全部数据，只需要将object类型的数据取出，然后进行后续分析即可obj_df = df.select_dtypes(include=['object']).copy()obj_df.head() 12# 在进行下一步处理的之前，需要将数据进行缺失值的处理，对列进行处理axis=1obj_df[obj_df.isnull().any(axis=1)] 12# 处理缺失值的方式有很多种，根据项目的不同或者填补缺失值或者去掉该样本。本文中的数据缺失用该列的众数来补充。obj_df.num_doors.value_counts() four 114 two 89 Name: num_doors, dtype: int64 1obj_df=obj_df.fillna(&#123;"num_doors":"four"&#125;) 在处理完缺失值之后，有以下几种方式进行category数据转化encoding Find and Replace label encoding One Hot encoding Custom Binary encoding sklearn advanced Approaches 12345678910# pandas里面的replace文档非常丰富，笔者在使用该功能时候，深感其参数众多，深感提供的功能也非常的强大# 本文中使用replace的功能，创建map的字典，针对需要数据清理的列进行清理更加方便，例如：cleanup_nums= &#123; "num_doors":&#123;"four":4,"two":2&#125;, "num_cylinders":&#123; "four":4,"six":6,"five":5,"eight":8,"two":2,"twelve":12,"three":3 &#125;&#125;obj_df.replace(cleanup_nums,inplace=True)obj_df.head() label encoding 是将一组无规则的，没有大小比较的数据转化为数字 比如body_style 字段中含有多个数据值，可以使用该方法将其转化 convertible &gt; 0 hardtop &gt; 1 hatchback &gt; 2 sedan &gt; 3 wagon &gt; 4 这种方式就像是密码编码一样，这，个比喻很有意思，就像之前看电影，记得一句台词，他们俩亲密的像做贼一样123# 通过pandas里面的 category数据类型，可以很方便的或者该编码obj_df["body_style"]=obj_df["body_style"].astype("category")obj_df.dtypes make object fuel_type object aspiration object num_doors int64 body_style category drive_wheels object engine_location object engine_type object num_cylinders int64 fuel_system object dtype: object 1234# 我们可以通过赋值新的列，保存其对应的code# 通过这种方法可以舒服的数据，便于以后的数据分析以及整理obj_df["body_style_code"] = obj_df["body_style"].cat.codesobj_df.head() one hot encoding label encoding 因为将wagon转化为4，而convertible变成了0，这里面是不是会有大大小的比较，可能会造成误解，然后利用one hot encoding这种方式是将特征转化为0或者1，这样会增加数据的列的数量，同时也减少了label encoding造成的衡量数据大小的误解。 pandas中提供了get_dummies 方法可以将需要转化的列的值转化为0,1,两种编码 12345# 新生成DataFrame包含了新生成的三列数据,# drive_wheels_4wd # drive_wheels_fwd# drive_wheels_rwdpd.get_dummies(obj_df,columns=["drive_wheels"]).head() 123# 该方法之所以强大，是因为可以同时处理多个category的列，同时选择prefix前缀分别对应好# 产生的新的DataFrame所有数据都包含pd.get_dummies(obj_df, columns=["body_style", "drive_wheels"], prefix=["body", "drive"]).head() 自定义0,1 encoding 有的时候回根据业务需要，可能会结合label encoding以及not hot 两种方式进行二值化。 1obj_df["engine_type"].value_counts() ohc 148 ohcf 15 ohcv 13 dohc 12 l 12 rotor 4 dohcv 1 Name: engine_type, dtype: int64 1234# 有的时候为了区分出 engine_type是否是och技术的，可以使用二值化，将该列进行处理# 这也突出了领域知识是如何以最有效的方式解决问题obj_df["engine_type_code"] = np.where(obj_df["engine_type"].str.contains("ohc"),1,0)obj_df[["make","engine_type","engine_type_code"]].head() scikit-learn中的数据转化 sklearn.processing模块提供了很多方便的数据转化以及缺失值处理方式。可以直接从该模块导入 Imputer LabelEncoder， LabelBinarizer，0,1归一化(最大最小标准化)， Normalizer正则化（L1，L2）一般用的不多， StandardScale 标准化 max_mixScale（最大最小标准化max_mix）， 非线性转换包括，生成多项式特征(PolynomialFeatures),将每个特征缩放在同样的范围或分布情况下 sklearn processing 模块官网文档链接 category_encoders包官方文档 至此，数据预处理以及category转化大致讲完了。]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>data processing</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据类型转化]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96%2F</url>
    <content type="text"><![CDATA[数据处理过程的数据类型 当利用pandas进行数据处理的时候，经常会遇到数据类型的问题，当拿到数据的时候，首先需要确定拿到的是正确类型的数据，一般通过数据类型的转化，这篇文章就介绍pandas里面的数据类型（data types也就是常用的dtyps），以及pandas与numpy之间的数据对应关系。 主要介绍object，int64，float64，datetime64，bool等几种类型，category与timedelta两种类型会单独的在其他文章中进行介绍。当然本文中也会涉及简单的介绍。 数据类型的问题一般都是出了问题之后才会发现的，所以有了一些经验之后就会拿到数据之后，就直接看数据类型，是否与自己想要处理的数据格式一致，这样可以从一开始避免一些尴尬的问题出现。那么我们以一个简单的例子，利用jupyter notebook进行一个数据类型的介绍。1234567####按照惯例导入两个常用的数据处理的包，numpy与pandasimport numpy as npimport pandas as pd# 从csv文件读取数据，数据表格中只有5行，里面包含了float，string，int三种数据python类型，也就是分别对应的pandas的float64，object，int64# csv文件中共有六列，第一列是表头，其余是数据。df = pd.read_csv("sales_data_types.csv")print(df) 1df.dtypes Customer Number float64 Customer Name object 2016 object 2017 object Percent Growth object Jan Units object Month int64 Day int64 Year int64 Active object dtype: object 123# 假如想得到2016年与2017年的数据总和，可以尝试,但并不是我们需要的答案，因为这两列中的数据类型是object，执行该操作之后，得到是一个更加长的字符串，# 当然我们可以通过df.info() 来获得关于数据框的更多的详细信息，df['2016']+df['2017'] 0 $125,000.00 $162,500.00 1 $920,000.00 $1,012,000.00 2 $50,000.00 $62,500.00 3 $350,000.00 $490,000.00 4 $15,000.00 $12,750.00 dtype: object 12345678910df.info()# Customer Number 列是float64，然而应该是int64# 2016 2017两列的数据是object，并不是float64或者int64格式# Percent以及Jan Units 也是objects而不是数字格式# Month，Day以及Year应该转化为datetime64[ns]格式# Active 列应该是布尔值# 如果不做数据清洗，很难进行下一步的数据分析，为了进行数据格式的转化，pandas里面有三种比较常用的方法# 1. astype()强制转化数据类型# 2. 通过创建常用的函数进行数据转化# 3. pandas提供的to_nueric()以及to_datetime() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 10 columns): Customer Number 5 non-null float64 Customer Name 5 non-null object 2016 5 non-null object 2017 5 non-null object Percent Growth 5 non-null object Jan Units 5 non-null object Month 5 non-null int64 Day 5 non-null int64 Year 5 non-null int64 Active 5 non-null object dtypes: float64(1), int64(3), object(6) memory usage: 480.0+ bytes 首先介绍最常用的astype()比如可以通过astype()将第一列的数据转化为整数int类型12df['Customer Number'].astype("int")# 这样的操作并没有改变原始的数据框，而只是返回的一个拷贝 0 10002 1 552278 2 23477 3 24900 4 651029 Name: Customer Number, dtype: int32 1234# 想要真正的改变数据框，通常需要通过赋值来进行，比如df["Customer Number"] = df["Customer Number"].astype("int")print("--------"*10)print(df.dtypes) Customer Number Customer Name 2016 2017 \ 0 10002 Quest Industries $125,000.00 $162,500.00 1 552278 Smith Plumbing $920,000.00 $1,012,000.00 2 23477 ACME Industrial $50,000.00 $62,500.00 3 24900 Brekke LTD $350,000.00 $490,000.00 4 651029 Harbor Co $15,000.00 $12,750.00 Percent Growth Jan Units Month Day Year Active 0 30.00% 500 1 10 2015 Y 1 10.00% 700 6 15 2014 Y 2 25.00% 125 3 29 2016 Y 3 4.00% 75 10 27 2015 Y 4 -15.00% Closed 2 2 2014 N -------------------------------------------------------------------------------- Customer Number int32 Customer Name object 2016 object 2017 object Percent Growth object Jan Units object Month int64 Day int64 Year int64 Active object dtype: object 12# 通过赋值在原始的数据框基础上进行了数据转化，可以重新看一下我们新生成的数据框print(df) 123# 然后像2016,2017 Percent Growth，Jan Units 这几列带有特殊符号的object是不能直接通过astype("flaot)方法进行转化的，# 这与python中的字符串转化为浮点数，都要求原始的字符都只能含有数字本身，不能含有其他的特殊字符# 我们可以试着将将Active列转化为布尔值，看一下到底会发生什么,五个结果全是True，说明并没有起到什么作用 1#df["Active"].astype("bool") 1df['2016'].astype('float') --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-145-47cc9d68cd65&gt; in &lt;module&gt;() ----&gt; 1 df[&apos;2016&apos;].astype(&apos;float&apos;) C:\Anaconda3\lib\site-packages\pandas\core\generic.py in astype(self, dtype, copy, raise_on_error, **kwargs) 3052 # else, only a single dtype is given 3053 new_data = self._data.astype(dtype=dtype, copy=copy, -&gt; 3054 raise_on_error=raise_on_error, **kwargs) 3055 return self._constructor(new_data).__finalize__(self) 3056 C:\Anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, **kwargs) 3187 3188 def astype(self, dtype, **kwargs): -&gt; 3189 return self.apply(&apos;astype&apos;, dtype=dtype, **kwargs) 3190 3191 def convert(self, **kwargs): C:\Anaconda3\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs) 3054 3055 kwargs[&apos;mgr&apos;] = self -&gt; 3056 applied = getattr(b, f)(**kwargs) 3057 result_blocks = _extend_blocks(applied, result_blocks) 3058 C:\Anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, copy, raise_on_error, values, **kwargs) 459 **kwargs): 460 return self._astype(dtype, copy=copy, raise_on_error=raise_on_error, --&gt; 461 values=values, **kwargs) 462 463 def _astype(self, dtype, copy=False, raise_on_error=True, values=None, C:\Anaconda3\lib\site-packages\pandas\core\internals.py in _astype(self, dtype, copy, raise_on_error, values, klass, mgr, **kwargs) 502 503 # _astype_nansafe works fine with 1-d only --&gt; 504 values = _astype_nansafe(values.ravel(), dtype, copy=True) 505 values = values.reshape(self.shape) 506 C:\Anaconda3\lib\site-packages\pandas\types\cast.py in _astype_nansafe(arr, dtype, copy) 535 536 if copy: --&gt; 537 return arr.astype(dtype) 538 return arr.view(dtype) 539 ValueError: could not convert string to float: &apos;$15,000.00 &apos; 以上的问题说明了一些问题 如果数据是纯净的数据，可以转化为数字 astype基本也就是两种用作，数字转化为单纯字符串，单纯数字的字符串转化为数字，含有其他的非数字的字符串是不能通过astype进行转化的。 需要引入其他的方法进行转化，也就有了下面的自定义函数方法 通过自定义函数清理数据 通过下面的函数可以将货币进行转化 123456789def convert_currency(var): """ convert the string number to a float _ 去除$ - 去除逗号， - 转化为浮点数类型 """ new_value = var.replace(",","").replace("$","") return float(new_value) 123# 通过replace函数将$以及逗号去掉，然后字符串转化为浮点数，让pandas选择pandas认为合适的特定类型，float或者int，该例子中将数据转化为了float64# 通过pandas中的apply函数将2016列中的数据全部转化df["2016"].apply(convert_currency) 0 125000.0 1 920000.0 2 50000.0 3 350000.0 4 15000.0 Name: 2016, dtype: float64 12# 当然可以通过lambda 函数将这个比较简单的函数一行带过df["2016"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64") 0 125000.0 1 920000.0 2 50000.0 3 350000.0 4 15000.0 Name: 2016, dtype: float64 12#同样可以利用lambda表达式将PercentGrowth进行数据清理df["Percent Growth"].apply(lambda x: x.replace("%","")).astype("float")/100 0 0.30 1 0.10 2 0.25 3 0.04 4 -0.15 Name: Percent Growth, dtype: float64 12345# 同样可以通过自定义函数进行解决，结果同上# 最后一个自定义函数是利用np.where() function 将Active 列转化为布尔值。df["Active"] = np.where(df["Active"] == "Y", True, False)df["Active"] 0 True 1 True 2 True 3 True 4 False Name: Active, dtype: bool 12345# 此时可查看一下数据格式df["2016"]=df["2016"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64")df["2017"]=df["2017"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64")df["Percent Growth"]=df["Percent Growth"].apply(lambda x: x.replace("%","")).astype("float")/100df.dtypes Customer Number int32 Customer Name object 2016 float64 2017 float64 Percent Growth float64 Jan Units object Month int64 Day int64 Year int64 Active bool dtype: object 123# 再次查看DataFrame# 此时只有Jan Units中格式需要转化，以及年月日的合并，可以利用pandas中自带的几个函数进行处理print(df) 利用pandas中函数进行处理12# pandas中pd.to_numeric()处理Jan Units中的数据pd.to_numeric(df["Jan Units"],errors='coerce').fillna(0) 0 500.0 1 700.0 2 125.0 3 75.0 4 0.0 Name: Jan Units, dtype: float64 12# 最后利用pd.to_datatime()将年月日进行合并pd.to_datetime(df[['Month', 'Day', 'Year']]) 0 2015-01-10 1 2014-06-15 2 2016-03-29 3 2015-10-27 4 2014-02-02 dtype: datetime64[ns] 123# 做到这里不要忘记重新赋值，否则原始数据并没有变化df["Jan Units"] = pd.to_numeric(df["Jan Units"],errors='coerce')df["Start_date"] = pd.to_datetime(df[['Month', 'Day', 'Year']]) 1df 1df.dtypes Customer Number int32 Customer Name object 2016 float64 2017 float64 Percent Growth float64 Jan Units float64 Month int64 Day int64 Year int64 Active bool Start_date datetime64[ns] dtype: object 1234567891011121314151617# 将这些转化整合在一起def convert_percent(val): """ Convert the percentage string to an actual floating point percent - Remove % - Divide by 100 to make decimal """ new_val = val.replace('%', '') return float(new_val) / 100df_2 = pd.read_csv("sales_data_types.csv",dtype=&#123;"Customer_Number":"int"&#125;,converters=&#123; "2016":convert_currency, "2017":convert_currency, "Percent Growth":convert_percent, "Jan Units":lambda x:pd.to_numeric(x,errors="coerce"), "Active":lambda x: np.where(x=="Y",True,False)&#125;) 12df_2.dtypes# Customer Number int64 Customer Name object 2016 float64 2017 float64 Percent Growth object Jan Units float64 Month int64 Day int64 Year int64 Active bool dtype: object 1df_2 至此，pandas里面数据类型目前还有timedelta以及category两个,之后会着重介绍category类型，这是类型是参考了R中的category设计的，在pandas 0.16 之后添加的，之后还会根据需要进行整理pandas的常用方法。]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>data processing</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic regression]]></title>
    <url>%2F2018%2F07%2F16%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[第三章 使用sklearn 实现机学习的分类算法分类算法 分类器的性能与计算能力和预测性能很大程度上取决于用于模型训练的数据 训练机器学习算法的五个步骤： 特征的选择 确定评价性能的标准 选择分类器及其优化算法 对模型性能的评估 算法的调优 sklearn初步使用 3.1 sklearn中包括的processing 模块中的标准化类，StandardScaler对特征进行标准化处理123456789from sklearn.processing import StandardSaclersc = StandardScaler() #实例化sc.fit(X_train)sc.transform(X_train)# - 以上两句可以并写成一句sc.fit_transform(X_trian)# - 我们使用相同的放缩参数分别对训练和测试数据集以保证他们的值是彼此相当的。**但是在使用fit_transform 只能对训练集使用，而测试机则只使用fit即可。**# - sklearn中的metrics类中包含了很多的评估参数，其中accuracy_score,# - 中accuracy_score(y_test,y_pred)，也就是那y_test与预测值相比较，得出正确率y_pred = model.predict(X_test-std) 过拟合现象过拟合现象出现有两个原因： 训练集与测试集特征分布不一致（黑天鹅和白天鹅） 模型训练的太过复杂，而样本量不足。同时针对两个原因而出现的解决方法: 收集多样化的样本 简化模型 交叉检验 逻辑斯谛回归感知机的一个最大缺点是：在样本不是完全线性可分的情况下，它永远不会收敛。分类算中的另一个简单高效的方法：logistics regression（分类模型） 很多情况下，我们会将逻辑回归的输出映射到二元分类问题的解决方案，需要确保逻辑回归的输出始终落在在0-1之间，此时S型函数的输出值正好满足了这个条件，其中： 几率比（odd ratio）特定的事件的发生的几率，用数学公式表示为：$\frac{p}{1-p} $，其中p为正事件的概率，不一定是有利的事件，而是我们将要预测的事件。以一个患者患有某种疾病的概率，我们可以将正事件的类标标记为y=1。 也就是样本特征与权重的线性组合，其计算公式： z = w·x + b 预测得到的概率可以通过一个量化器（单位阶跃函数）简单的转化为二元输出 如果y＞0.5 则判断该样本类别为1，如y＜0.5，则判定该样本是其他类别。 对应上面的展开式，如果z≥0，则判断类别是1，否则是其他。 阈值也就是0.5 通过逻辑斯谛回归模型的代价函数获得权重 判定某个样本属于类别1或者0 的条件概率如下： 逻辑回回归的代价函数是最小二乘损失函数 为了推导出逻辑斯蒂回归的代价函数，需要先定义一个极大似然函数L, 用极大似然估计来根据给定的训练集估计出参数w,对上式两边取对数，化简为求极大似然函数的最大值等价于求-l(w)的最小值，即： 利用梯度下降法求参数 在开始梯度下降之前，sigmoid function有一个很好的性质，梯度的负方向就是代价函数下降最快的方向，借助泰勒展开，可以得到（函数可微，可导）其中，f’(x) 和δ为向量，那么这两者的内积就等于当θ=π时，也就是在δ与f’(x)的方向相反时，取得最小值， 也就是下降的最快的方向了这里也就是: f(x+δ) - f(x) = - ||δ||·||f’(x)||也就是 其中，wj表示第j个特征的权重，η为学习率，用来控制步长。 对损失函数J(w))中的w的第j个权重求偏导，所以，在使用梯度下降法更新权重时，只要根据公式当样本量极大的时候，每次更新权重需要耗费大量的算力，这时可以采取随机梯度下降法，这时，每次迭代的时候需要将样本重新打乱，然后用下面的式子更新权重 参考文献: Raschka S. Python Machine Learning[M]. Packt Publishing, 2015 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[general regression models]]></title>
    <url>%2F2018%2F07%2F11%2Fgeneral-regression-models%2F</url>
    <content type="text"><![CDATA[1.1.1. Ordinary Least Squares123from sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit([[0,0],[1,1],[2,2]],[0,1,2]) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1reg.coef_ array([0.5, 0.5]) 最小二乘法的代价函数表述为： 1.1.2 Ridge Regression岭回归通过对最小二乘法的系数做出惩罚以解决部分的问题，最小化了惩罚的残差平方和 现行回归含有惩罚项的代价函数表述为： 正则化的背后的概念是引入额外的信息（偏差）来对极端参数的权重做出惩罚，此处的正则化则是引入的L2正则化。 代价函数的参数α的变化导致权重稀疏的变化，岭回归即L2正则化（L2收缩），也叫权重衰减： 123from sklearn import linear_modelreg= linear_model.Ridge(alpha=0.5)reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&apos;auto&apos;, tol=0.001) 1reg.intercept_ 0.1363636363636364 1reg.coef_ array([0.34545455, 0.34545455]) 1.1.2.1 Setting the regularization parameter: generalized Cross-Validation 通过交叉验证获得回归效果最恰当的惩罚项的参数123from sklearn import linear_modelreg=linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None, store_cv_values=False) 1reg.alpha_ 0.1 1.1.3 Lasso(权重稀疏） L1正则化可生成稀疏的特征向量，且大多数的权值为0，当高维的数据集中包含许多不想管的特征，尤其是在不相关的特征数量大于样本数量是，权重的稀疏化可以发挥特征选择的作用。 损失函数可以表示为： 123from sklearn import linear_modelreg= linear_model.Lasso(alpha=0.1)reg.fit([[0,0],[1,1]],[0,1]) Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 123import numpy as npreg.predict([[1,1]])# 如果是一维数组的话，需要在外面再加一层中括号，或者 array([0.8]) 1reg Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 1reg.coef_ array([0.6, 0. ]) 1reg.intercept_ 0.2 12]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging和Boosting的概念与区别]]></title>
    <url>%2F2018%2F06%2F11%2FHello-hexo%2F</url>
    <content type="text"><![CDATA[Bagging和Boosting的概念与区别随机森林属于集成学习(ensemble learning)中的bagging算法，在集成算法中主要分为bagging算法与boosting算法 Bagging算法(套袋法，bootstrap aggregating) bagging的算法过程如下： 从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。 对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是决策树，knn等） 对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。 Boosting（提升法） boosting的算法过程如下： 对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。 同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。) Bagging和Boosting 的主要区别 样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样的权重。 样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大 预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。 并行计算: Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成. 将决策树与以上框架组合成新的算法 Bagging + 决策树 = 随机森林 AdaBoost + 决策树 = 提升树 gradient + 决策树 = （梯度提升树）GDBT 决策树 常用的决策树有ID3， C4.5 ,CART三种. 三种算法模型构架相似，只是采用了不同的指标 首先看ID3算法 基于奥卡姆剃刀原理，即用尽量较少的东西做更多的事。ID3算法即iterative Dichotomiser3，迭代二叉树三代，越是小型的决策树优于较大的决策树。 核心思想是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分类。 信息增益是属性选择中一个重要指标，它定义为一个属性能够为分类系统带来的多少信息，带来的信息越多，该属性就越重要，而信息量，就是熵。 熵的定义是信息量的期望值，熵越大，一个变量的不确定性越大，它带来的信息量就越大，计算信息熵的公式为：，其中，p为出现c分类时的概率。 如何计算一个属性的信息增益？]]></content>
      <categories>
        <category>ensemble method</category>
      </categories>
      <tags>
        <tag>boosting</tag>
        <tag>集成算法</tag>
      </tags>
  </entry>
</search>
