<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pandas data processing]]></title>
    <url>%2F2018%2F08%2F02%2Fdata-processing-pandas%2F</url>
    <content type="text"><![CDATA[category type data in pandas 实际应用pandas过程中，经常会用到category数据类型，通常以text的形式显示，包括颜色（红，绿，蓝），尺寸的大小（大，中，小），还有地理信息等（国家，省份），这些数据的处理经常会有各种各样的问题，pandas以及scikit-learn两个包可以将category数据转化为合适的数值型格式，这篇主要介绍通过这两个包处理category类型的数据转化为数值类型，也就是encoding的过程。 数据来源UCI Machine Learning Repository，这个数据集中包含了很多的category类型的数据，可以从链接汇总查看数据的代表的含义。 下面开始导入需要用到的包 12import numpy as npimport pandas as pd 12345678910# 规定一下数据列的各个名称，headers = ["symboling", "normalized_losses", "make", "fuel_type", "aspiration", "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base", "length", "width", "height", "curb_weight", "engine_type", "num_cylinders", "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price"]# 从pandas导入csv文件，将?标记为NaN缺失值df=pd.read_csv("http://mlr.cs.umass.edu/ml/machine-learning-databases/autos/imports-85.data",header=None,names=headers,na_values="?")df.head() 1df.dtypes symboling int64 normalized_losses float64 make object fuel_type object aspiration object num_doors object body_style object drive_wheels object engine_location object wheel_base float64 length float64 width float64 height float64 curb_weight int64 engine_type object num_cylinders object engine_size int64 fuel_system object bore float64 stroke float64 compression_ratio float64 horsepower float64 peak_rpm float64 city_mpg int64 highway_mpg int64 price float64 dtype: object 123# 如果只关注category 类型的数据，其实根本没有必要拿到这些全部数据，只需要将object类型的数据取出，然后进行后续分析即可obj_df = df.select_dtypes(include=['object']).copy()obj_df.head() 12# 在进行下一步处理的之前，需要将数据进行缺失值的处理，对列进行处理axis=1obj_df[obj_df.isnull().any(axis=1)] 12# 处理缺失值的方式有很多种，根据项目的不同或者填补缺失值或者去掉该样本。本文中的数据缺失用该列的众数来补充。obj_df.num_doors.value_counts() four 114 two 89 Name: num_doors, dtype: int64 1obj_df=obj_df.fillna(&#123;"num_doors":"four"&#125;) 在处理完缺失值之后，有以下几种方式进行category数据转化encoding Find and Replace label encoding One Hot encoding Custom Binary encoding sklearn advanced Approaches 12345678910# pandas里面的replace文档非常丰富，笔者在使用该功能时候，深感其参数众多，深感提供的功能也非常的强大# 本文中使用replace的功能，创建map的字典，针对需要数据清理的列进行清理更加方便，例如：cleanup_nums= &#123; "num_doors":&#123;"four":4,"two":2&#125;, "num_cylinders":&#123; "four":4,"six":6,"five":5,"eight":8,"two":2,"twelve":12,"three":3 &#125;&#125;obj_df.replace(cleanup_nums,inplace=True)obj_df.head() label encoding 是将一组无规则的，没有大小比较的数据转化为数字 比如body_style 字段中含有多个数据值，可以使用该方法将其转化 convertible &gt; 0 hardtop &gt; 1 hatchback &gt; 2 sedan &gt; 3 wagon &gt; 4 这种方式就像是密码编码一样，这，个比喻很有意思，就像之前看电影，记得一句台词，他们俩亲密的像做贼一样123# 通过pandas里面的 category数据类型，可以很方便的或者该编码obj_df["body_style"]=obj_df["body_style"].astype("category")obj_df.dtypes make object fuel_type object aspiration object num_doors int64 body_style category drive_wheels object engine_location object engine_type object num_cylinders int64 fuel_system object dtype: object 1234# 我们可以通过赋值新的列，保存其对应的code# 通过这种方法可以舒服的数据，便于以后的数据分析以及整理obj_df["body_style_code"] = obj_df["body_style"].cat.codesobj_df.head() one hot encoding label encoding 因为将wagon转化为4，而convertible变成了0，这里面是不是会有大大小的比较，可能会造成误解，然后利用one hot encoding这种方式是将特征转化为0或者1，这样会增加数据的列的数量，同时也减少了label encoding造成的衡量数据大小的误解。 pandas中提供了get_dummies 方法可以将需要转化的列的值转化为0,1,两种编码 12345# 新生成DataFrame包含了新生成的三列数据,# drive_wheels_4wd # drive_wheels_fwd# drive_wheels_rwdpd.get_dummies(obj_df,columns=["drive_wheels"]).head() 123# 该方法之所以强大，是因为可以同时处理多个category的列，同时选择prefix前缀分别对应好# 产生的新的DataFrame所有数据都包含pd.get_dummies(obj_df, columns=["body_style", "drive_wheels"], prefix=["body", "drive"]).head() 自定义0,1 encoding 有的时候回根据业务需要，可能会结合label encoding以及not hot 两种方式进行二值化。 1obj_df["engine_type"].value_counts() ohc 148 ohcf 15 ohcv 13 dohc 12 l 12 rotor 4 dohcv 1 Name: engine_type, dtype: int64 1234# 有的时候为了区分出 engine_type是否是och技术的，可以使用二值化，将该列进行处理# 这也突出了领域知识是如何以最有效的方式解决问题obj_df["engine_type_code"] = np.where(obj_df["engine_type"].str.contains("ohc"),1,0)obj_df[["make","engine_type","engine_type_code"]].head() scikit-learn中的数据转化 sklearn.processing模块提供了很多方便的数据转化以及缺失值处理方式。可以直接从该模块导入 Imputer LabelEncoder， LabelBinarizer，0,1归一化(最大最小标准化)， Normalizer正则化（L1，L2）一般用的不多， StandardScale 标准化 max_mixScale（最大最小标准化max_mix）， 非线性转换包括，生成多项式特征(PolynomialFeatures),将每个特征缩放在同样的范围或分布情况下 sklearn processing 模块官网文档链接 category_encoders包官方文档 至此，数据预处理以及category转化大致讲完了。]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>data processing</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据类型转化]]></title>
    <url>%2F2018%2F08%2F02%2F%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96%2F</url>
    <content type="text"><![CDATA[数据处理过程的数据类型 当利用pandas进行数据处理的时候，经常会遇到数据类型的问题，当拿到数据的时候，首先需要确定拿到的是正确类型的数据，一般通过数据类型的转化，这篇文章就介绍pandas里面的数据类型（data types也就是常用的dtyps），以及pandas与numpy之间的数据对应关系。 主要介绍object，int64，float64，datetime64，bool等几种类型，category与timedelta两种类型会单独的在其他文章中进行介绍。当然本文中也会涉及简单的介绍。 数据类型的问题一般都是出了问题之后才会发现的，所以有了一些经验之后就会拿到数据之后，就直接看数据类型，是否与自己想要处理的数据格式一致，这样可以从一开始避免一些尴尬的问题出现。那么我们以一个简单的例子，利用jupyter notebook进行一个数据类型的介绍。1234567####按照惯例导入两个常用的数据处理的包，numpy与pandasimport numpy as npimport pandas as pd# 从csv文件读取数据，数据表格中只有5行，里面包含了float，string，int三种数据python类型，也就是分别对应的pandas的float64，object，int64# csv文件中共有六列，第一列是表头，其余是数据。df = pd.read_csv("sales_data_types.csv")print(df) 1df.dtypes Customer Number float64 Customer Name object 2016 object 2017 object Percent Growth object Jan Units object Month int64 Day int64 Year int64 Active object dtype: object 123# 假如想得到2016年与2017年的数据总和，可以尝试,但并不是我们需要的答案，因为这两列中的数据类型是object，执行该操作之后，得到是一个更加长的字符串，# 当然我们可以通过df.info() 来获得关于数据框的更多的详细信息，df['2016']+df['2017'] 0 $125,000.00 $162,500.00 1 $920,000.00 $1,012,000.00 2 $50,000.00 $62,500.00 3 $350,000.00 $490,000.00 4 $15,000.00 $12,750.00 dtype: object 12345678910df.info()# Customer Number 列是float64，然而应该是int64# 2016 2017两列的数据是object，并不是float64或者int64格式# Percent以及Jan Units 也是objects而不是数字格式# Month，Day以及Year应该转化为datetime64[ns]格式# Active 列应该是布尔值# 如果不做数据清洗，很难进行下一步的数据分析，为了进行数据格式的转化，pandas里面有三种比较常用的方法# 1. astype()强制转化数据类型# 2. 通过创建常用的函数进行数据转化# 3. pandas提供的to_nueric()以及to_datetime() &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 5 entries, 0 to 4 Data columns (total 10 columns): Customer Number 5 non-null float64 Customer Name 5 non-null object 2016 5 non-null object 2017 5 non-null object Percent Growth 5 non-null object Jan Units 5 non-null object Month 5 non-null int64 Day 5 non-null int64 Year 5 non-null int64 Active 5 non-null object dtypes: float64(1), int64(3), object(6) memory usage: 480.0+ bytes 首先介绍最常用的astype()比如可以通过astype()将第一列的数据转化为整数int类型12df['Customer Number'].astype("int")# 这样的操作并没有改变原始的数据框，而只是返回的一个拷贝 0 10002 1 552278 2 23477 3 24900 4 651029 Name: Customer Number, dtype: int32 1234# 想要真正的改变数据框，通常需要通过赋值来进行，比如df["Customer Number"] = df["Customer Number"].astype("int")print("--------"*10)print(df.dtypes) Customer Number Customer Name 2016 2017 \ 0 10002 Quest Industries $125,000.00 $162,500.00 1 552278 Smith Plumbing $920,000.00 $1,012,000.00 2 23477 ACME Industrial $50,000.00 $62,500.00 3 24900 Brekke LTD $350,000.00 $490,000.00 4 651029 Harbor Co $15,000.00 $12,750.00 Percent Growth Jan Units Month Day Year Active 0 30.00% 500 1 10 2015 Y 1 10.00% 700 6 15 2014 Y 2 25.00% 125 3 29 2016 Y 3 4.00% 75 10 27 2015 Y 4 -15.00% Closed 2 2 2014 N -------------------------------------------------------------------------------- Customer Number int32 Customer Name object 2016 object 2017 object Percent Growth object Jan Units object Month int64 Day int64 Year int64 Active object dtype: object 12# 通过赋值在原始的数据框基础上进行了数据转化，可以重新看一下我们新生成的数据框print(df) 123# 然后像2016,2017 Percent Growth，Jan Units 这几列带有特殊符号的object是不能直接通过astype("flaot)方法进行转化的，# 这与python中的字符串转化为浮点数，都要求原始的字符都只能含有数字本身，不能含有其他的特殊字符# 我们可以试着将将Active列转化为布尔值，看一下到底会发生什么,五个结果全是True，说明并没有起到什么作用 1#df["Active"].astype("bool") 1df['2016'].astype('float') --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-145-47cc9d68cd65&gt; in &lt;module&gt;() ----&gt; 1 df[&apos;2016&apos;].astype(&apos;float&apos;) C:\Anaconda3\lib\site-packages\pandas\core\generic.py in astype(self, dtype, copy, raise_on_error, **kwargs) 3052 # else, only a single dtype is given 3053 new_data = self._data.astype(dtype=dtype, copy=copy, -&gt; 3054 raise_on_error=raise_on_error, **kwargs) 3055 return self._constructor(new_data).__finalize__(self) 3056 C:\Anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, **kwargs) 3187 3188 def astype(self, dtype, **kwargs): -&gt; 3189 return self.apply(&apos;astype&apos;, dtype=dtype, **kwargs) 3190 3191 def convert(self, **kwargs): C:\Anaconda3\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs) 3054 3055 kwargs[&apos;mgr&apos;] = self -&gt; 3056 applied = getattr(b, f)(**kwargs) 3057 result_blocks = _extend_blocks(applied, result_blocks) 3058 C:\Anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, copy, raise_on_error, values, **kwargs) 459 **kwargs): 460 return self._astype(dtype, copy=copy, raise_on_error=raise_on_error, --&gt; 461 values=values, **kwargs) 462 463 def _astype(self, dtype, copy=False, raise_on_error=True, values=None, C:\Anaconda3\lib\site-packages\pandas\core\internals.py in _astype(self, dtype, copy, raise_on_error, values, klass, mgr, **kwargs) 502 503 # _astype_nansafe works fine with 1-d only --&gt; 504 values = _astype_nansafe(values.ravel(), dtype, copy=True) 505 values = values.reshape(self.shape) 506 C:\Anaconda3\lib\site-packages\pandas\types\cast.py in _astype_nansafe(arr, dtype, copy) 535 536 if copy: --&gt; 537 return arr.astype(dtype) 538 return arr.view(dtype) 539 ValueError: could not convert string to float: &apos;$15,000.00 &apos; 以上的问题说明了一些问题 如果数据是纯净的数据，可以转化为数字 astype基本也就是两种用作，数字转化为单纯字符串，单纯数字的字符串转化为数字，含有其他的非数字的字符串是不能通过astype进行转化的。 需要引入其他的方法进行转化，也就有了下面的自定义函数方法 通过自定义函数清理数据 通过下面的函数可以将货币进行转化 123456789def convert_currency(var): """ convert the string number to a float _ 去除$ - 去除逗号， - 转化为浮点数类型 """ new_value = var.replace(",","").replace("$","") return float(new_value) 123# 通过replace函数将$以及逗号去掉，然后字符串转化为浮点数，让pandas选择pandas认为合适的特定类型，float或者int，该例子中将数据转化为了float64# 通过pandas中的apply函数将2016列中的数据全部转化df["2016"].apply(convert_currency) 0 125000.0 1 920000.0 2 50000.0 3 350000.0 4 15000.0 Name: 2016, dtype: float64 12# 当然可以通过lambda 函数将这个比较简单的函数一行带过df["2016"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64") 0 125000.0 1 920000.0 2 50000.0 3 350000.0 4 15000.0 Name: 2016, dtype: float64 12#同样可以利用lambda表达式将PercentGrowth进行数据清理df["Percent Growth"].apply(lambda x: x.replace("%","")).astype("float")/100 0 0.30 1 0.10 2 0.25 3 0.04 4 -0.15 Name: Percent Growth, dtype: float64 12345# 同样可以通过自定义函数进行解决，结果同上# 最后一个自定义函数是利用np.where() function 将Active 列转化为布尔值。df["Active"] = np.where(df["Active"] == "Y", True, False)df["Active"] 0 True 1 True 2 True 3 True 4 False Name: Active, dtype: bool 12345# 此时可查看一下数据格式df["2016"]=df["2016"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64")df["2017"]=df["2017"].apply(lambda x: x.replace(",","").replace("$","")).astype("float64")df["Percent Growth"]=df["Percent Growth"].apply(lambda x: x.replace("%","")).astype("float")/100df.dtypes Customer Number int32 Customer Name object 2016 float64 2017 float64 Percent Growth float64 Jan Units object Month int64 Day int64 Year int64 Active bool dtype: object 123# 再次查看DataFrame# 此时只有Jan Units中格式需要转化，以及年月日的合并，可以利用pandas中自带的几个函数进行处理print(df) 利用pandas中函数进行处理12# pandas中pd.to_numeric()处理Jan Units中的数据pd.to_numeric(df["Jan Units"],errors='coerce').fillna(0) 0 500.0 1 700.0 2 125.0 3 75.0 4 0.0 Name: Jan Units, dtype: float64 12# 最后利用pd.to_datatime()将年月日进行合并pd.to_datetime(df[['Month', 'Day', 'Year']]) 0 2015-01-10 1 2014-06-15 2 2016-03-29 3 2015-10-27 4 2014-02-02 dtype: datetime64[ns] 123# 做到这里不要忘记重新赋值，否则原始数据并没有变化df["Jan Units"] = pd.to_numeric(df["Jan Units"],errors='coerce')df["Start_date"] = pd.to_datetime(df[['Month', 'Day', 'Year']]) 1df 1df.dtypes Customer Number int32 Customer Name object 2016 float64 2017 float64 Percent Growth float64 Jan Units float64 Month int64 Day int64 Year int64 Active bool Start_date datetime64[ns] dtype: object 1234567891011121314151617# 将这些转化整合在一起def convert_percent(val): """ Convert the percentage string to an actual floating point percent - Remove % - Divide by 100 to make decimal """ new_val = val.replace('%', '') return float(new_val) / 100df_2 = pd.read_csv("sales_data_types.csv",dtype=&#123;"Customer_Number":"int"&#125;,converters=&#123; "2016":convert_currency, "2017":convert_currency, "Percent Growth":convert_percent, "Jan Units":lambda x:pd.to_numeric(x,errors="coerce"), "Active":lambda x: np.where(x=="Y",True,False)&#125;) 12df_2.dtypes# Customer Number int64 Customer Name object 2016 float64 2017 float64 Percent Growth object Jan Units float64 Month int64 Day int64 Year int64 Active bool dtype: object 1df_2 至此，pandas里面数据类型目前还有timedelta以及category两个,之后会着重介绍category类型，这是类型是参考了R中的category设计的，在pandas 0.16 之后添加的，之后还会根据需要进行整理pandas的常用方法。]]></content>
      <categories>
        <category>pandas</category>
      </categories>
      <tags>
        <tag>data processing</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic regression]]></title>
    <url>%2F2018%2F07%2F16%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[第三章 使用sklearn 实现机学习的分类算法分类算法 分类器的性能与计算能力和预测性能很大程度上取决于用于模型训练的数据 训练机器学习算法的五个步骤： 特征的选择 确定评价性能的标准 选择分类器及其优化算法 对模型性能的评估 算法的调优 sklearn初步使用 3.1 sklearn中包括的processing 模块中的标准化类，StandardScaler对特征进行标准化处理123456789from sklearn.processing import StandardSaclersc = StandardScaler() #实例化sc.fit(X_train)sc.transform(X_train)# - 以上两句可以并写成一句sc.fit_transform(X_trian)# - 我们使用相同的放缩参数分别对训练和测试数据集以保证他们的值是彼此相当的。**但是在使用fit_transform 只能对训练集使用，而测试机则只使用fit即可。**# - sklearn中的metrics类中包含了很多的评估参数，其中accuracy_score,# - 中accuracy_score(y_test,y_pred)，也就是那y_test与预测值相比较，得出正确率y_pred = model.predict(X_test-std) 过拟合现象过拟合现象出现有两个原因： 训练集与测试集特征分布不一致（黑天鹅和白天鹅） 模型训练的太过复杂，而样本量不足。同时针对两个原因而出现的解决方法: 收集多样化的样本 简化模型 交叉检验 逻辑斯谛回归感知机的一个最大缺点是：在样本不是完全线性可分的情况下，它永远不会收敛。分类算中的另一个简单高效的方法：logistics regression（分类模型） 很多情况下，我们会将逻辑回归的输出映射到二元分类问题的解决方案，需要确保逻辑回归的输出始终落在在0-1之间，此时S型函数的输出值正好满足了这个条件，其中： 几率比（odd ratio）特定的事件的发生的几率，用数学公式表示为：$\frac{p}{1-p} $，其中p为正事件的概率，不一定是有利的事件，而是我们将要预测的事件。以一个患者患有某种疾病的概率，我们可以将正事件的类标标记为y=1。 也就是样本特征与权重的线性组合，其计算公式： z = w·x + b 预测得到的概率可以通过一个量化器（单位阶跃函数）简单的转化为二元输出 如果y＞0.5 则判断该样本类别为1，如y＜0.5，则判定该样本是其他类别。 对应上面的展开式，如果z≥0，则判断类别是1，否则是其他。 阈值也就是0.5 通过逻辑斯谛回归模型的代价函数获得权重 判定某个样本属于类别1或者0 的条件概率如下： 逻辑回回归的代价函数是最小二乘损失函数 为了推导出逻辑斯蒂回归的代价函数，需要先定义一个极大似然函数L, 用极大似然估计来根据给定的训练集估计出参数w,对上式两边取对数，化简为求极大似然函数的最大值等价于求-l(w)的最小值，即： 利用梯度下降法求参数 在开始梯度下降之前，sigmoid function有一个很好的性质，梯度的负方向就是代价函数下降最快的方向，借助泰勒展开，可以得到（函数可微，可导）其中，f’(x) 和δ为向量，那么这两者的内积就等于当θ=π时，也就是在δ与f’(x)的方向相反时，取得最小值， 也就是下降的最快的方向了这里也就是: f(x+δ) - f(x) = - ||δ||·||f’(x)||也就是 其中，wj表示第j个特征的权重，η为学习率，用来控制步长。 对损失函数J(w))中的w的第j个权重求偏导，所以，在使用梯度下降法更新权重时，只要根据公式当样本量极大的时候，每次更新权重需要耗费大量的算力，这时可以采取随机梯度下降法，这时，每次迭代的时候需要将样本重新打乱，然后用下面的式子更新权重 参考文献: Raschka S. Python Machine Learning[M]. Packt Publishing, 2015 周志华. 机器学习 : = Machine learning[M]. 清华大学出版社, 2016.]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>logistic regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[general regression models]]></title>
    <url>%2F2018%2F07%2F11%2Fgeneral-regression-models%2F</url>
    <content type="text"><![CDATA[1.1.1. Ordinary Least Squares123from sklearn import linear_modelreg = linear_model.LinearRegression()reg.fit([[0,0],[1,1],[2,2]],[0,1,2]) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) 1reg.coef_ array([0.5, 0.5]) 最小二乘法的代价函数表述为： 1.1.2 Ridge Regression岭回归通过对最小二乘法的系数做出惩罚以解决部分的问题，最小化了惩罚的残差平方和 现行回归含有惩罚项的代价函数表述为： 正则化的背后的概念是引入额外的信息（偏差）来对极端参数的权重做出惩罚，此处的正则化则是引入的L2正则化。 代价函数的参数α的变化导致权重稀疏的变化，岭回归即L2正则化（L2收缩），也叫权重衰减： 123from sklearn import linear_modelreg= linear_model.Ridge(alpha=0.5)reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&apos;auto&apos;, tol=0.001) 1reg.intercept_ 0.1363636363636364 1reg.coef_ array([0.34545455, 0.34545455]) 1.1.2.1 Setting the regularization parameter: generalized Cross-Validation 通过交叉验证获得回归效果最恰当的惩罚项的参数123from sklearn import linear_modelreg=linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])reg.fit([[0,0],[0,0],[1,1]],[0,0.1,1]) RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None, normalize=False, scoring=None, store_cv_values=False) 1reg.alpha_ 0.1 1.1.3 Lasso(权重稀疏） L1正则化可生成稀疏的特征向量，且大多数的权值为0，当高维的数据集中包含许多不想管的特征，尤其是在不相关的特征数量大于样本数量是，权重的稀疏化可以发挥特征选择的作用。 损失函数可以表示为： 123from sklearn import linear_modelreg= linear_model.Lasso(alpha=0.1)reg.fit([[0,0],[1,1]],[0,1]) Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 123import numpy as npreg.predict([[1,1]])# 如果是一维数组的话，需要在外面再加一层中括号，或者 array([0.8]) 1reg Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False) 1reg.coef_ array([0.6, 0. ]) 1reg.intercept_ 0.2 12]]></content>
      <categories>
        <category>regression</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging和Boosting的概念与区别]]></title>
    <url>%2F2018%2F06%2F11%2FHello-hexo%2F</url>
    <content type="text"><![CDATA[Bagging和Boosting的概念与区别随机森林属于集成学习(ensemble learning)中的bagging算法，在集成算法中主要分为bagging算法与boosting算法 Bagging算法(套袋法，bootstrap aggregating) bagging的算法过程如下： 从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。 对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是决策树，knn等） 对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。 Boosting（提升法） boosting的算法过程如下： 对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。 同时加大分类 误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。) Bagging和Boosting 的主要区别 样本选择上: Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样的权重。 样本权重上：Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大 预测函数上：Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。 并行计算: Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成. 将决策树与以上框架组合成新的算法 Bagging + 决策树 = 随机森林 AdaBoost + 决策树 = 提升树 gradient + 决策树 = （梯度提升树）GDBT 决策树 常用的决策树有ID3， C4.5 ,CART三种. 三种算法模型构架相似，只是采用了不同的指标 首先看ID3算法 基于奥卡姆剃刀原理，即用尽量较少的东西做更多的事。ID3算法即iterative Dichotomiser3，迭代二叉树三代，越是小型的决策树优于较大的决策树。 核心思想是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分类。 信息增益是属性选择中一个重要指标，它定义为一个属性能够为分类系统带来的多少信息，带来的信息越多，该属性就越重要，而信息量，就是熵。 熵的定义是信息量的期望值，熵越大，一个变量的不确定性越大，它带来的信息量就越大，计算信息熵的公式为：，其中，p为出现c分类时的概率。 如何计算一个属性的信息增益？]]></content>
      <categories>
        <category>ensemble method</category>
      </categories>
      <tags>
        <tag>boosting</tag>
        <tag>集成算法</tag>
      </tags>
  </entry>
</search>
